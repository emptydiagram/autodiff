{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94681d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "mm = importlib.import_module(\"makemore-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa25c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_split(words, B):\n",
    "  xs = []\n",
    "  ys = []\n",
    "  start_idx = mm.CHAR_INDICES['.']\n",
    "  \n",
    "  for word in words:\n",
    "    # initial context_array = [0, 0, 0]\n",
    "    context_array = [start_idx] * B\n",
    "    xs.append(context_array.copy())\n",
    "    word = f'{word}.'\n",
    "\n",
    "    # loop invariant:\n",
    "    #   - xs has a sequence of inputs already processed (possibly empty),\n",
    "    #     followed by the next input to be processed\n",
    "    #   - xs = [x_1, ..., x_{k-1}, x_k]^T\n",
    "    #   - ys = [y_1, ..., y_{k-1}]^T has a sequence of outputs, one for each\n",
    "    #     of the inputs already processed.\n",
    "    for ch in word:\n",
    "      ch_idx = mm.CHAR_INDICES[ch]\n",
    "      ys.append(ch_idx)\n",
    "      # assuming training data contains no \".\"'s and has properly been filtered out,\n",
    "      # then the only way ch_idx == 0 is if we're at the end\n",
    "      if ch_idx != 0:\n",
    "        context_array.pop(0)\n",
    "        context_array.append(ch_idx)\n",
    "        xs.append(context_array.copy())\n",
    "\n",
    "  X = torch.tensor(xs)\n",
    "  Y = torch.tensor(ys)\n",
    "  print(X.shape, Y.shape)\n",
    "  return (X, Y)\n",
    "\n",
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6154a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182512, 3]) torch.Size([182512])\n",
      "torch.Size([22864, 3]) torch.Size([22864])\n",
      "torch.Size([22770, 3]) torch.Size([22770])\n",
      "228146\n"
     ]
    }
   ],
   "source": [
    "words = mm.load_words_from_file('names.txt')\n",
    "\n",
    "# 80/10/10 split\n",
    "B = 3\n",
    "M = len(words)\n",
    "random.seed(12345)\n",
    "random.shuffle(words)\n",
    "X_tr, Y_tr = make_dataset_split(words[:int(0.8 * M)], B)\n",
    "X_val, Y_val = make_dataset_split(words[int(0.8 * M):int(0.9 * M)], B)\n",
    "X_tst, Y_tst = make_dataset_split(words[int(0.9 * M):], B)\n",
    "print(X_tr.shape[0] + X_val.shape[0] + X_tst.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78d833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of params: 6297\n"
     ]
    }
   ],
   "source": [
    "# linear -> batch norm -> tanh -> linear\n",
    "\n",
    "size_vocab = 27\n",
    "size_embed = 10\n",
    "size_hidden = 100\n",
    "\n",
    "RAND_SEED = 1729\n",
    "gen = torch.Generator().manual_seed(RAND_SEED)\n",
    "\n",
    "C = torch.randn((size_vocab, size_embed), generator=gen)\n",
    "W1 = torch.randn((B*size_embed, size_hidden), generator=gen) * 0.1\n",
    "b1 = torch.randn((size_hidden,), generator=gen) * 0.1\n",
    "W2 = torch.randn((size_hidden, size_vocab), generator=gen) * 0.1\n",
    "b2 = torch.randn((size_vocab,), generator=gen) * 0.1\n",
    "bn_bias = torch.randn((1, size_hidden), generator=gen) * 0.1\n",
    "bn_gain = torch.randn((1, size_hidden), generator=gen) * 0.1 + 1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_bias, bn_gain]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "print(f\"total # of params: {sum([p.nelement() for p in parameters])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a107cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_batch = 32\n",
    "N = size_batch\n",
    "b_idxs = torch.randint(0, X_tr.shape[0], (size_batch,), generator=gen)\n",
    "X_b, Y_b = X_tr[b_idxs], Y_tr[b_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b48fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/dev/me/venv-nh-nn-zero-to-hero/lib/python3.8/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.4346, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X_b].view(X_b.shape[0], -1)\n",
    "l1_act = embeddings @ W1 + b1\n",
    "\n",
    "bn_eps = 1e-5\n",
    "l1_mean = l1_act.mean(0, keepdim=True)\n",
    "bn_diff = l1_act - l1_mean\n",
    "bn_diff_sq = bn_diff**2\n",
    "# unbiased sample variance\n",
    "bn_var = 1./(size_batch - 1) * bn_diff_sq.sum(0, keepdim=True)\n",
    "bn_var_inv = (bn_var + bn_eps)**-0.5\n",
    "bn_act = bn_gain * (bn_diff * bn_var_inv) + bn_bias\n",
    "\n",
    "nl_act = torch.tanh(bn_act)\n",
    "\n",
    "logits = nl_act @ W2 + b2\n",
    "\n",
    "logits_max = logits.max(1, keepdim=True).values\n",
    "counts = torch.exp(logits - logits_max)\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "\n",
    "neg_log_probs = -torch.log(probs)\n",
    "loss = neg_log_probs[range(size_batch), Y_b].mean()\n",
    "\n",
    "\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "\n",
    "for v in [neg_log_probs, probs, counts_sum_inv, counts_sum, counts, logits_max, logits, nl_act, bn_act,\n",
    "         bn_var_inv, bn_var, bn_diff_sq, bn_diff, l1_mean, l1_act, embeddings]:\n",
    "  v.retain_grad()\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89633b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_neg_log_probs | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_probs         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_counts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_counts_sum    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "d_counts        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "d_neg_log_probs = torch.zeros((size_batch, size_vocab))\n",
    "d_neg_log_probs[range(size_batch), Y_b] = 1./size_batch\n",
    "\n",
    "d_probs = d_neg_log_probs * (-1. / probs)\n",
    "d_counts_sum_inv = (d_probs * counts).sum(1, keepdim=True)\n",
    "d_counts_sum = d_counts_sum_inv * (- counts_sum**-2)\n",
    "d_counts = d_probs * counts_sum_inv + torch.ones((size_batch, size_vocab)) * d_counts_sum\n",
    "\n",
    "cmp('d_neg_log_probs', d_neg_log_probs, neg_log_probs)\n",
    "cmp('d_probs', d_probs, probs)\n",
    "cmp('d_counts_sum_inv', d_counts_sum_inv, counts_sum_inv)\n",
    "cmp('d_counts_sum', d_counts_sum, counts_sum)\n",
    "cmp('d_counts', d_counts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d40d3e",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
