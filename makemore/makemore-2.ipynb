{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ad2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "mm = importlib.import_module(\"makemore-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3353a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da96d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = mm.load_words_from_file('names.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe4f71",
   "metadata": {},
   "source": [
    "# what are we trying to do\n",
    "\n",
    "add more context in training data. we don't want input-output pairs to just be $(c_k, c_{k+1})$, but rather $((c_{k-B+1}, \\ldots, c_k), c_{k+1})$ for context length $B$.\n",
    "\n",
    "From our set of words, build up a training set of character sequences $(c_{k-B+1}, \\ldots, c_{k-1}) \\mapsto c_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c898710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182512, 3]) torch.Size([182512])\n",
      "torch.Size([22864, 3]) torch.Size([22864])\n",
      "torch.Size([22770, 3]) torch.Size([22770])\n",
      "228146\n"
     ]
    }
   ],
   "source": [
    "start_idx = mm.CHAR_INDICES['.']\n",
    "B = 3\n",
    "\n",
    "def make_dataset_split(words):\n",
    "  xs = []\n",
    "  ys = []\n",
    "  \n",
    "  for word in words:\n",
    "    # initial context_array = [0, 0, 0]\n",
    "    context_array = [start_idx] * B\n",
    "    xs.append(context_array.copy())\n",
    "    word = f'{word}.'\n",
    "\n",
    "    # loop invariant:\n",
    "    #   - xs has a sequence of inputs already processed (possibly empty),\n",
    "    #     followed by the next input to be processed\n",
    "    #   - xs = [x_1, ..., x_{k-1}, x_k]^T\n",
    "    #   - ys = [y_1, ..., y_{k-1}]^T has a sequence of outputs, one for each\n",
    "    #     of the inputs already processed.\n",
    "    for ch in word:\n",
    "      ch_idx = mm.CHAR_INDICES[ch]\n",
    "      ys.append(ch_idx)\n",
    "      # assuming training data contains no \".\"'s and has properly been filtered out,\n",
    "      # then the only way ch_idx == 0 is if we're at the end\n",
    "      if ch_idx != 0:\n",
    "        context_array.pop(0)\n",
    "        context_array.append(ch_idx)\n",
    "        xs.append(context_array.copy())\n",
    "\n",
    "  X = torch.tensor(xs)\n",
    "  Y = torch.tensor(ys)\n",
    "  print(X.shape, Y.shape)\n",
    "  return (X, Y)\n",
    "\n",
    "# 80/10/10 split\n",
    "M = len(words)\n",
    "random.seed(12345)\n",
    "random.shuffle(words)\n",
    "X_tr, Y_tr = make_dataset_split(words[:int(0.8 * M)])\n",
    "X_val, Y_val = make_dataset_split(words[int(0.8 * M):int(0.9 * M)])\n",
    "X_tst, Y_tst = make_dataset_split(words[int(0.9 * M):])\n",
    "print(X_tr.shape[0] + X_val.shape[0] + X_tst.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c8c05",
   "metadata": {},
   "source": [
    "# Using an MLP w/ character embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb9c35",
   "metadata": {},
   "source": [
    "We build an n-gram probability model: a model that calculates the conditional probability of the next character $\\textbf{c}_t$ given the previously seen characters $(\\textbf{c}_{t-d+1}, \\ldots, \\textbf{c}_{t-1})$:\n",
    "\n",
    "$$P(\\textbf{c}_t | \\textbf{c}_{t-1}, \\textbf{c}_{t-2}, \\ldots, \\textbf{c}_{t-d+1})$$\n",
    "\n",
    "using\n",
    "\n",
    "1. a character embedding layer\n",
    "2. a hidden layer with a non-linearity (tanh activation) \n",
    "3. a softmax layer\n",
    "\n",
    "(NOTE: below, using \"$e$\" to be a variable, $\\exp$ will be the exponential function. sorry not sorry.) the layers are defined by:\n",
    "\n",
    " - $c =$ the current sequence of characters\n",
    " - $e = C(c)$\n",
    " - $h = \\text{tanh}((W^{(1)})^T e + b^{(1)})$\n",
    " - $l = (W^{(2)})^T x + b^{(1)}$\n",
    " - $p = \\text{softmax}(l)$\n",
    " - $\\text{loss} = \\text{Cross-Entropy}(p)$\n",
    " \n",
    "$$c \\mapsto e \\mapsto h \\mapsto l \\mapsto p$$\n",
    "\n",
    "and we calculate the loss for each input's output probability vector $p$.\n",
    " \n",
    "some notation:\n",
    "\n",
    " - block size $B$ ($B = 3$ here)\n",
    " - a single, shared character embedding $C: \\text{Chars } \\to R^d$\n",
    "    - $R^d$ is the embedding space\n",
    "    - here, $d = 2$\n",
    " - $K$ is the size of the hidden layer output (here $K = 100$)\n",
    " - $W^{(1)} \\in \\mathbb{R}^{(Bd) \\times K}$, and $b^{(1)} \\in \\mathbb{R}^K$\n",
    " - $W^{(2)} \\in \\mathbb{R}^{K \\times 27}$, and $b^{(2)} \\in \\mathbb{R}^{27}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9526ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 15032\n"
     ]
    }
   ],
   "source": [
    "d = 15\n",
    "K = 200\n",
    "\n",
    "C = torch.randn((27,d))\n",
    "\n",
    "g = torch.Generator().manual_seed(12345)\n",
    "W1 = torch.randn((B*d, K), generator=g)\n",
    "b1 = torch.randn((K,), generator=g)\n",
    "W2 = torch.randn((K, 27), generator=g)\n",
    "b2 = torch.randn((27,), generator=g)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "for prm in params:\n",
    "  prm.requires_grad = True\n",
    "\n",
    "\n",
    "print(f\"number of parameters = {sum([p.nelement() for p in params])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "410efb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss = 26.658891677856445\n",
      "[2000] loss = 3.1221680641174316\n",
      "[4000] loss = 3.2022035121917725\n",
      "[6000] loss = 2.4897313117980957\n",
      "[8000] loss = 2.6937429904937744\n",
      "[10000] loss = 2.49950909614563\n",
      "[12000] loss = 2.3878071308135986\n",
      "[14000] loss = 2.222501039505005\n",
      "[16000] loss = 2.305237054824829\n",
      "[18000] loss = 2.1167891025543213\n",
      "[20000] loss = 2.2838659286499023\n",
      "[22000] loss = 2.0340373516082764\n",
      "[24000] loss = 2.178699016571045\n",
      "[26000] loss = 2.402988910675049\n",
      "[28000] loss = 1.9995990991592407\n",
      "[30000] loss = 2.340970754623413\n",
      "[32000] loss = 2.1475155353546143\n",
      "[34000] loss = 2.2620625495910645\n",
      "[36000] loss = 2.079510450363159\n",
      "[38000] loss = 1.9154407978057861\n",
      "[40000] loss = 2.277647018432617\n",
      "[42000] loss = 2.3154256343841553\n",
      "[44000] loss = 2.051833391189575\n",
      "[46000] loss = 2.1735739707946777\n",
      "[48000] loss = 2.0750677585601807\n",
      "[50000] loss = 2.297797679901123\n",
      "[52000] loss = 2.0699045658111572\n",
      "[54000] loss = 2.08777117729187\n",
      "[56000] loss = 2.177191972732544\n",
      "[58000] loss = 2.1318113803863525\n",
      "[60000] loss = 2.15922474861145\n",
      "[62000] loss = 2.063559055328369\n",
      "[64000] loss = 2.1468505859375\n",
      "[66000] loss = 2.2146270275115967\n",
      "[68000] loss = 2.0153417587280273\n",
      "[70000] loss = 1.9836959838867188\n",
      "[72000] loss = 2.1640782356262207\n",
      "[74000] loss = 2.1460506916046143\n",
      "[76000] loss = 2.0164883136749268\n",
      "[78000] loss = 2.038100481033325\n",
      "[80000] loss = 2.195284366607666\n",
      "[82000] loss = 2.019580841064453\n",
      "[84000] loss = 2.1408851146698\n",
      "[86000] loss = 2.139178514480591\n",
      "[88000] loss = 2.0448660850524902\n",
      "[90000] loss = 2.1472835540771484\n",
      "[92000] loss = 2.1217997074127197\n",
      "[94000] loss = 1.9485881328582764\n",
      "[96000] loss = 1.7852710485458374\n",
      "[98000] loss = 2.043891429901123\n",
      "[100000] loss = 2.03411865234375\n",
      "[102000] loss = 2.0284323692321777\n",
      "[104000] loss = 1.9659773111343384\n",
      "[106000] loss = 1.9911715984344482\n",
      "[108000] loss = 2.119967222213745\n",
      "[110000] loss = 2.0497217178344727\n",
      "[112000] loss = 2.0262680053710938\n",
      "[114000] loss = 2.033579111099243\n",
      "[116000] loss = 2.003783702850342\n",
      "[118000] loss = 2.0324606895446777\n",
      "[120000] loss = 1.982242226600647\n",
      "[122000] loss = 1.9329315423965454\n",
      "[124000] loss = 1.9845582246780396\n",
      "[126000] loss = 2.087831735610962\n",
      "[128000] loss = 2.172699451446533\n",
      "[130000] loss = 2.1336581707000732\n",
      "[132000] loss = 2.0741963386535645\n",
      "[134000] loss = 2.1942591667175293\n",
      "[136000] loss = 1.9840680360794067\n",
      "[138000] loss = 2.057628870010376\n",
      "[140000] loss = 2.0004281997680664\n",
      "[142000] loss = 1.937037467956543\n",
      "[144000] loss = 2.173346757888794\n",
      "[146000] loss = 2.1013786792755127\n",
      "[148000] loss = 2.182089328765869\n",
      "[150000] loss = 1.989806890487671\n",
      "[152000] loss = 2.2107038497924805\n",
      "[154000] loss = 2.028208017349243\n",
      "[156000] loss = 2.0867271423339844\n",
      "[158000] loss = 2.121453046798706\n",
      "[160000] loss = 2.0435211658477783\n",
      "[162000] loss = 2.0938594341278076\n",
      "[164000] loss = 1.9946304559707642\n",
      "[166000] loss = 1.9449694156646729\n",
      "[168000] loss = 1.9740777015686035\n",
      "[170000] loss = 2.031167507171631\n",
      "[172000] loss = 1.9359729290008545\n",
      "[174000] loss = 1.8925750255584717\n",
      "[176000] loss = 2.186131477355957\n",
      "[178000] loss = 2.235694646835327\n",
      "[180000] loss = 2.2529327869415283\n",
      "[182000] loss = 2.076333999633789\n",
      "[184000] loss = 2.0377354621887207\n",
      "[186000] loss = 2.1521730422973633\n",
      "[188000] loss = 2.0973167419433594\n",
      "[190000] loss = 2.0703651905059814\n",
      "[192000] loss = 1.8472758531570435\n",
      "[194000] loss = 2.0819499492645264\n",
      "[196000] loss = 2.054940938949585\n",
      "[198000] loss = 2.0336241722106934\n",
      "[200000] loss = 2.1924304962158203\n"
     ]
    }
   ],
   "source": [
    "num_iters = 200000\n",
    "get_learning_rate = lambda it: 0.1 if it < 100000 else 0.01\n",
    "batch_size = 128\n",
    "\n",
    "for it in range(num_iters):\n",
    "  batch_idxs = torch.randint(0, X_tr.shape[0], (batch_size,))\n",
    "  X_batch = X_tr[batch_idxs]\n",
    "  Y_batch = Y_tr[batch_idxs]\n",
    "  \n",
    "  # forward propagation, calculate outputs and loss\n",
    "  embeddings = C[X_batch].view(-1, B*d)\n",
    "  hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "  logits = hiddens @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, Y_batch)\n",
    "  \n",
    "  if it % 2000 == 0:\n",
    "    print(f\"[{it}] loss = {loss.item()}\")\n",
    "\n",
    "  # backward propagate the error gradient\n",
    "  for prm in params:\n",
    "    prm.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for prm in params:\n",
    "    prm.data += -get_learning_rate(it) * prm.grad\n",
    "\n",
    "print(f\"[{num_iters}] loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb7aa4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.063495635986328"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X_tr].view(-1, B*d)\n",
    "hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "logits = hiddens @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y_tr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "668655bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.129589319229126"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X_val].view(-1, B*d)\n",
    "hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "logits = hiddens @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y_val)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee62c31",
   "metadata": {},
   "source": [
    "# tune learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "lr_exps = torch.linspace(-3, 0.5, num_iters)\n",
    "lrs = 10 ** lr_exps\n",
    "losses = []\n",
    "batch_size = 32\n",
    "\n",
    "for it in range(num_iters):\n",
    "  batch_idxs = torch.randint(0, X_tr.shape[0], (batch_size,))\n",
    "  X_batch = X_tr[batch_idxs]\n",
    "  Y_batch = Y_tr[batch_idxs]\n",
    "  \n",
    "  # forward propagation, calculate outputs and loss\n",
    "  embeddings = C[X_batch].view(-1, B*d)\n",
    "  hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "  logits = hiddens @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, Y_batch)\n",
    "  \n",
    "  if it % 200 == 0:\n",
    "    print(f\"[{it}] loss = {loss.item()}\")\n",
    "    \n",
    "  losses.append(loss.item())\n",
    "\n",
    "  # backward propagate the error gradient\n",
    "  for prm in params:\n",
    "    prm.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for prm in params:\n",
    "    prm.data += -lrs[it] * prm.grad\n",
    "    \n",
    "\n",
    "print(f\"final loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_exps, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4beb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
