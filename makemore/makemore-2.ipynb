{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ad2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "mm = importlib.import_module(\"makemore-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3353a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da96d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = mm.load_words_from_file('names.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe4f71",
   "metadata": {},
   "source": [
    "# what are we trying to do\n",
    "\n",
    "add more context in training data. we don't want input-output pairs to just be $(c_k, c_{k+1})$, but rather $((c_{k-B+1}, \\ldots, c_k), c_{k+1})$ for context length $B$.\n",
    "\n",
    "From our set of words, build up a training set of character sequences $(c_{k-B+1}, \\ldots, c_{k-1}) \\mapsto c_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c898710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3]) torch.int64 torch.Size([228146]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "start_idx = mm.CHAR_INDICES['.']\n",
    "context_length = 3\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for word in words:\n",
    "  # initial context_array = [0, 0, 0]\n",
    "  context_array = [start_idx] * context_length\n",
    "  xs.append(context_array.copy())\n",
    "  word = f'{word}.'\n",
    "\n",
    "  # loop invariant:\n",
    "  #   - xs has a sequence of inputs already processed (possibly empty),\n",
    "  #     followed by the next input to be processed\n",
    "  #   - xs = [x_1, ..., x_{k-1}, x_k]^T\n",
    "  #   - ys = [y_1, ..., y_{k-1}]^T has a sequence of outputs, one for each\n",
    "  #     of the inputs already processed.\n",
    "  for ch in word:\n",
    "    ch_idx = mm.CHAR_INDICES[ch]\n",
    "    ys.append(ch_idx)\n",
    "    # assuming training data contains no \".\"'s and has properly been filtered out,\n",
    "    # then the only way ch_idx == 0 is if we're at the end\n",
    "    if ch_idx != 0:\n",
    "      context_array.pop(0)\n",
    "      context_array.append(ch_idx)\n",
    "      xs.append(context_array.copy())\n",
    "\n",
    "X = torch.tensor(xs)\n",
    "Y = torch.tensor(ys)\n",
    "# number of samples\n",
    "M = X.shape[0]\n",
    "\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c8c05",
   "metadata": {},
   "source": [
    "# Using an MLP w/ character embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb9c35",
   "metadata": {},
   "source": [
    "We build an n-gram probability model: a model that calculates the conditional probability of the next character $\\textbf{c}_t$ given the previously seen characters $(\\textbf{c}_{t-d+1}, \\ldots, \\textbf{c}_{t-1})$:\n",
    "\n",
    "$$P(\\textbf{c}_t | \\textbf{c}_{t-1}, \\textbf{c}_{t-2}, \\ldots, \\textbf{c}_{t-d+1})$$\n",
    "\n",
    "using\n",
    "\n",
    "1. a character embedding layer\n",
    "2. a hidden layer with a non-linearity (tanh activation) \n",
    "3. a softmax layer\n",
    "\n",
    "(NOTE: below, using \"$e$\" to be a variable, $\\exp$ will be the exponential function. sorry not sorry.) the layers are defined by:\n",
    "\n",
    " - $c =$ the current sequence of characters\n",
    " - $e = C(c)$\n",
    " - $h = \\text{tanh}((W^{(1)})^T e + b^{(1)})$\n",
    " - $l = (W^{(2)})^T x + b^{(1)}$\n",
    " - $p = \\text{softmax}(l)$\n",
    " - $\\text{loss} = \\text{Cross-Entropy}(p)$\n",
    " \n",
    "$$c \\mapsto e \\mapsto h \\mapsto l \\mapsto p$$\n",
    "\n",
    "and we calculate the loss for each input's output probability vector $p$.\n",
    " \n",
    "some notation:\n",
    "\n",
    " - block size $B$ ($B = 3$ here)\n",
    " - a single, shared character embedding $C: \\text{Chars } \\to R^d$\n",
    "    - $R^d$ is the embedding space\n",
    "    - here, $d = 2$\n",
    " - $K$ is the size of the hidden layer output (here $K = 100$)\n",
    " - $W^{(1)} \\in \\mathbb{R}^{(Bd) \\times K}$, and $b^{(1)} \\in \\mathbb{R}^K$\n",
    " - $W^{(2)} \\in \\mathbb{R}^{K \\times 27}$, and $b^{(2)} \\in \\mathbb{R}^{27}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9526ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 3427\n"
     ]
    }
   ],
   "source": [
    "B = 3\n",
    "d = 2\n",
    "K = 100\n",
    "\n",
    "C = torch.randn((27,d))\n",
    "\n",
    "g = torch.Generator().manual_seed(12345)\n",
    "W1 = torch.randn((B*d, K), generator=g)\n",
    "b1 = torch.randn((K,), generator=g)\n",
    "W2 = torch.randn((K, 27), generator=g)\n",
    "b2 = torch.randn((27,), generator=g)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "print(f\"number of parameters = {sum([p.nelement() for p in params])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "410efb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss = 3.256247043609619\n",
      "[200] loss = 2.4560561180114746\n",
      "[400] loss = 2.759335994720459\n",
      "[600] loss = 2.9781603813171387\n",
      "[800] loss = 2.8154613971710205\n",
      "[1000] loss = 2.553621768951416\n",
      "[1200] loss = 2.6483516693115234\n",
      "[1400] loss = 2.2547922134399414\n",
      "[1600] loss = 2.677513599395752\n",
      "[1800] loss = 2.326481819152832\n",
      "[2000] loss = 2.6013107299804688\n",
      "[2200] loss = 2.294363021850586\n",
      "[2400] loss = 2.4159796237945557\n",
      "[2600] loss = 2.6304101943969727\n",
      "[2800] loss = 2.252729892730713\n",
      "[3000] loss = 2.7548513412475586\n",
      "[3200] loss = 2.7321689128875732\n",
      "[3400] loss = 2.5576043128967285\n",
      "[3600] loss = 2.6842174530029297\n",
      "[3800] loss = 2.664486885070801\n",
      "[4000] loss = 2.970008134841919\n",
      "[4200] loss = 2.811039686203003\n",
      "[4400] loss = 2.688290596008301\n",
      "[4600] loss = 2.6198182106018066\n",
      "[4800] loss = 2.3921103477478027\n",
      "[5000] loss = 2.581737995147705\n",
      "[5200] loss = 2.501214027404785\n",
      "[5400] loss = 2.4199914932250977\n",
      "[5600] loss = 2.3907744884490967\n",
      "[5800] loss = 2.2113475799560547\n",
      "final loss = 2.8403849601745605\n"
     ]
    }
   ],
   "source": [
    "for prm in params:\n",
    "  prm.requires_grad = True\n",
    "  \n",
    "num_iters = 6000\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "for it in range(num_iters):\n",
    "  batch_idxs = torch.randint(0, X.shape[0], (batch_size,))\n",
    "  X_batch = X[batch_idxs]\n",
    "  Y_batch = Y[batch_idxs]\n",
    "  \n",
    "  # forward propagation, calculate outputs and loss\n",
    "  embeddings = C[X_batch].view(-1, B*d)\n",
    "  hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "  logits = hiddens @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, Y_batch)\n",
    "  \n",
    "  if it % 200 == 0:\n",
    "    print(f\"[{it}] loss = {loss.item()}\")\n",
    "\n",
    "  # backward propagate the error gradient\n",
    "  for prm in params:\n",
    "    prm.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for prm in params:\n",
    "    prm.data += -learning_rate * prm.grad\n",
    "    \n",
    "\n",
    "print(f\"final loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "668655bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4954025745391846"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X].view(-1, B*d)\n",
    "hiddens = torch.tanh(embeddings @ W1 + b1)\n",
    "logits = hiddens @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56087b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
